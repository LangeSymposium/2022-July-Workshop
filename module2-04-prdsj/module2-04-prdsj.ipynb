{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbd506a",
   "metadata": {},
   "source": [
    "# Practicing Reproducible Data Science in Julia\n",
    "\n",
    "**Acknowledgements**\n",
    "\n",
    "- @oxinabox and contributors for [DataDeps.jl](https://github.com/oxinabox/DataDeps.jl). See the related paper [here](https://openresearchsoftware.metajnl.com/article/10.5334/jors.244/).\n",
    "- @JackDunnNZ for [UCIData.jl](https://github.com/JackDunnNZ/UCIData.jl), which uses DataDeps.jl. This helped me understand the usefulness of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66ab03",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Why DataDeps.jl?](#Why-DataDeps.jl?)\n",
    "1. [Overview](#Overview)\n",
    "1. [Creating a Data Dependency](#Creating-a-Data-Dependency)\n",
    "    1. [Example 1: Create a reproducible pipeline to retrieve public datasets](#Example-1:-Creating-a-reproducible-pipeline-to-retrieve-public-datasets)\n",
    "    1. [Example 2: Include a preprocessing step](#Example-2:-Include-a-preprocessing-step)\n",
    "    1. [Example 3: Reproducing datasets derived from simulations](#Example-3:-Reproducing-datasets-derived-from-simulations)\n",
    "1. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7545993",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdab8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0b1ac",
   "metadata": {},
   "source": [
    "# Why DataDeps.jl?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3880b9",
   "metadata": {},
   "source": [
    "Abstract:\n",
    "\n",
    "> We present DataDeps.jl: a julia package for the **reproducible handling of static datasets to enhance\n",
    "the repeatability of scripts used in the data and computational sciences**.\n",
    "> It is used to automate the data setup part of running software which accompanies a paper to replicate a result.\n",
    "> This step is commonly done manually, which expends time and allows for confusion.\n",
    "> This functionality is also useful for other packages which require data to function (e.g. a trained machine learning based model).\n",
    "> DataDeps.jl simplifies extending research software by automatically managing the dependencies and makes it easier\n",
    "to run another author’s code, thus enhancing the reproducibility of data science research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fc383",
   "metadata": {},
   "source": [
    "<img src=\"misc/datadeps-paper-1.1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025b7ac",
   "metadata": {},
   "source": [
    "### Why not store the data in Git?\n",
    "*Source*: [DataDeps.jl documentation](https://www.oxinabox.net/DataDeps.jl/stable/#Why-not-store-the-data-in-Git?-1)\n",
    "\n",
    "**Git is good for files that meet 3 requirements**:\n",
    "\n",
    "- Plain text (not binary)\n",
    "- Smallish (Github will not accept files >50Mb in size)\n",
    "- Dynamic (Git is version control, it is good at knowing about changes)\n",
    "\n",
    "There is certainly some room around the edges for this, like storing a few images in the repository is OK, but storing all of [ImageNet](https://en.wikipedia.org/wiki/ImageNet) is a no go.\n",
    "\n",
    "**DataDeps.jl is good for**:\n",
    "\n",
    "- Any file format (binary is OK)\n",
    "- Any size\n",
    "- Static (that is to say it doesn't change; DataDeps.jl does not do version control)\n",
    "\n",
    "The main use case is downloading large datasets for machine learning, and corpora for NLP.\n",
    "\n",
    "**In this case the data is not even normally yours to begin with.**\n",
    "\n",
    "It lives on some website somewhere.\n",
    "\n",
    "**You don't want to copy and redistribute it; and depending on the license you may not even be allowed to.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304368a8",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62057c",
   "metadata": {},
   "source": [
    "The DataDeps.jl package makes it so that the special incantation\n",
    "\n",
    "```julia\n",
    "@datadep_str my_dataset # same as @datadep_str(mydataset)\n",
    "```\n",
    "\n",
    "automatically resolves to a local filepath where a dataset is stored.\n",
    "This means that\n",
    "\n",
    "- Your scripts are not littered with hardcoded paths just to access a data input into some analysis pipeline.\n",
    "- You do not spend time worrying about whether operating systems `xx` and `yy` will be able to handle the filepath.\n",
    "- You do not have to worry about where the dataset is stored. **If the object does not exist locally, DataDeps.jl uses a pre-specified pipeline to retrieve it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45800562",
   "metadata": {},
   "source": [
    "Let's look at the interface for the [`DataDep`](https://www.oxinabox.net/DataDeps.jl/stable/z40-apiref/#DataDeps.DataDep) type:\n",
    "\n",
    "```julia\n",
    "DataDep(\n",
    "    name::String,\n",
    "    message::String,\n",
    "    remote_path::Union{String,Vector{String}...},\n",
    "    [checksum::Union{String,Vector{String}...},]; # Optional, generated if not provided\n",
    "    # Optional keyword arguments\n",
    "    fetch_method=fetch_default # (remote_filepath, local_directory)->local_filepath\n",
    "    post_fetch_method=identity # (local_filepath)->Any\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b325afd",
   "metadata": {},
   "source": [
    "- Every `DataDep` has a `name`, which corresponds to a *parent* folder where data is to be stored.\n",
    "\n",
    "\n",
    "\n",
    "- Prior to downloading, a `message` is displayed to a user. This is a good place to make any acknowledgemets regarding the source, including important URLs, **papers that should be cited**, links to additional info, and license (when applicable). Importantly, users must consent to downloading the data at this stage.\n",
    "\n",
    "\n",
    "\n",
    "- The `remote_path` is the source associated with a `DataDep`. This can be a single path or multiple paths when given as a list.\n",
    "\n",
    "\n",
    "\n",
    "- The `checksum` is verified after downloading to guarantee that files associated with a `DataDep` are the same as those specified by the creator of the registration block. DataDeps.jl will help you generate a `checksum` that you can then paste into your registration block. Can be an single checksum or multiple checksums applied to each file in `remote_path`.\n",
    "\n",
    "    Not strictly required, but it is good practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3d4d2",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you just want to include a dataset in your project, without any preprocessing, the first four arguments are all you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aeac1f",
   "metadata": {},
   "source": [
    "### Customizing the download process\n",
    "\n",
    "The `fetch_method` is a function that lets you override the initial download step at `remote_filepath`. Given a `local_directory`, this function must determine a local file name and return that name at the end.\n",
    "\n",
    "By default, the `fetch_method` simply uses `HTTP.jl` to handle URLs or `Base.download` to handle other filepaths.\n",
    "\n",
    "**You want to customize this if**:\n",
    "\n",
    "- you need to use a transfer protocol not covered by Base Julia,\n",
    "\n",
    "- accessing the source requires some form of authentication, or\n",
    "\n",
    "- **your dataset is generated by a simulation you wish to reproduce locally**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72def446",
   "metadata": {},
   "source": [
    "The `post_fetch_method` is a function that allows one to manipulate the original data retrieved by the `fetch_method`.\n",
    "\n",
    "**You want to customize this if you want to do some processing of the data and use that version as the starting point in your project**. For example:\n",
    "    \n",
    "- If you are creating multiple `DataDep`s for tabular data in your project, you may want to set a standard format for column names (`x1`, `x2` or `gene1` `gene2`).\n",
    "\n",
    "- You want to screen for missing values, corrupted entries, and so on and have already determined a set of commands to do this automatically.\n",
    "\n",
    "- You want to apply some minor transformations to the data so that it is ready to use in your project immediately after loading it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674f536",
   "metadata": {},
   "source": [
    "# Creating a Data Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e85f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading, writing, manipulating, and compressing data\n",
    "using CSV, DataFrames, CodecZlib\n",
    "\n",
    "# Random number generation\n",
    "using Random, StableRNGs\n",
    "\n",
    "# Basic statistics functions\n",
    "using Statistics\n",
    "\n",
    "# Visualization\n",
    "using GLMakie\n",
    "\n",
    "# The star of the show: creating data dependencies\n",
    "using DataDeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f525bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify this environment variable to disable to default load paths in DataDeps.jl\n",
    "ENV[\"DATADEPS_NO_STANDARD_LOAD_PATH\"] = true\n",
    "\n",
    "# Specify the load path just for the purposes of this demo.\n",
    "if haskey(ENV, \"DATADEPS_LOAD_PATH\")\n",
    "    rm(ENV[\"DATADEPS_LOAD_PATH\"])\n",
    "end\n",
    "ENV[\"DATADEPS_LOAD_PATH\"] = mktempdir(\"/home/alanderos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327c0d0",
   "metadata": {},
   "source": [
    "## Example 1: Creating a reproducible pipeline to retrieve public datasets\n",
    "\n",
    "For this example we will use the famous `iris` data hosted by the [UCI Machine Learning Repository](https://archive.ics.uci.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadep_iris = DataDep(\n",
    "    # 1. Call this datset \"iris\".\n",
    "    \"iris\",\n",
    "    # 2. Set the message to display when downloading.\n",
    "    \"\"\"\n",
    "    Dataset: iris\n",
    "    Author: R. A. Fisher (donated by Michael Marshall)\n",
    "    \n",
    "    Observations: 150\n",
    "    Features:     4\n",
    "    Classes:      3\n",
    "    \n",
    "    Please see https://archive.ics.uci.edu/ml/datasets/iris for additional information.\n",
    "    \"\"\",\n",
    "    # 3. Set the remote_path used to download data.\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(datadep_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@datadep_str \"iris\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4e64a",
   "metadata": {},
   "source": [
    "Note the file information in the help message:\n",
    "\n",
    "> Do you want to download the dataset from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data to \"/home/alanderos/jl_81EctG/iris\"?\n",
    "\n",
    "This tells us that the file `iris.data` will be downloaded to a folder `iris` located in our DataDeps registry.\n",
    "\n",
    "Let's try reading in the file with CSV.jl and DataFrames.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adacd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(@datadep_str(\"iris/iris.data\"), DataFrame)\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe05b6",
   "metadata": {},
   "source": [
    "Note the columns. In this case CSV.jl accidentally detected a header. Fix this by specifying `header=false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f79623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(@datadep_str(\"iris/iris.data\"), DataFrame, header=false)\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c577e",
   "metadata": {},
   "source": [
    "The download information also mentioned a missing checksum:\n",
    "\n",
    "> ┌ Warning: Checksum not provided, add to the Datadep Registration the following hash line\n",
    ">\n",
    "> │   hash = 6f608b71a7317216319b4d27b4d9bc84e6abd734eda7872b71a458569e2656c0\n",
    ">\n",
    "> └ @ DataDeps /home/alanderos/.julia/packages/DataDeps/EDWdQ/src/verification.jl:44\n",
    "\n",
    "Let's try recreating the `DataDep` with the checksum and running the process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadep_iris = DataDep(\n",
    "    # 1. Call this datset \"iris\".\n",
    "    \"iris\",\n",
    "    # 2. Set the message to display when downloading.\n",
    "    \"\"\"\n",
    "    Dataset: iris\n",
    "    Author: R. A. Fisher (donated by Michael Marshall)\n",
    "    \n",
    "    Observations: 150\n",
    "    Features:     4\n",
    "    Classes:      3\n",
    "    \n",
    "    Please see https://archive.ics.uci.edu/ml/datasets/iris for additional information.\n",
    "    \"\"\",\n",
    "    # 3. Set the remote_path used to download data.\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "    # 4. Set the checksum this time.\n",
    "    \"6f608b71a7317216319b4d27b4d9bc84e6abd734eda7872b71a458569e2656c0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(datadep_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(@datadep_str(\"iris\"); recursive=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(@datadep_str(\"iris/iris.data\"), DataFrame, header=false)\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f291d",
   "metadata": {},
   "source": [
    "Everything work and there was no checksum warning this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055f490",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "We now have an easy way to access the `iris` dataset but there a few improvements we could make.\n",
    "\n",
    "**Problem**: Remembering that the filename is `iris.data` is a bit annoying.\n",
    "   \n",
    "- Create a wrapper function that automatically retrieves the filename for your dataset.\n",
    "- *Not in the scope of DataDeps.jl*.\n",
    "\n",
    "**Problem**: For our project, we always want to load as a `DataFrame` (or some other format).\n",
    "\n",
    "- Again, create a wrapper function that will always load as a `DataFrame`.\n",
    "- *Not in the scope of DataDeps.jl*.\n",
    "\n",
    "**Problem**: We want to reorder the columns so that all our `DataDeps` have a common format.\n",
    "\n",
    "- Implement a custom `post_fetch_method`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695d5ef",
   "metadata": {},
   "source": [
    "## Example 2: Include a preprocessing step\n",
    "\n",
    "This example uses breast cytology data for diagnosis of breast cancer. A summary is provided [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)).\n",
    "\n",
    "To demonstrate how DataDeps.jl handles multiple files and allows for preprocessing, we will\n",
    "\n",
    "1. Create a single `DataDep` to store two classification datasets, `bcw.csv` and `wdbc.csv`,\n",
    "1. Apply a standard format to both files so that data labels appear in the first column and predictors appear in subsequent columns. Column names should also have a standardized format.\n",
    "2. Change the data labels from `2` and `4` (or `B` and `M`) to `benign` and `malignant`.\n",
    "3. Store the metadata for column names is a separate `.info` file.\n",
    "\n",
    "We implement these steps in a custom `post_fetch_method`, which accepts a local filename as an argument:\n",
    "\n",
    "```julia\n",
    "function my_function(local_filepath)\n",
    "    # do something; no restrictions here\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ae909",
   "metadata": {},
   "source": [
    "First, we define a function to help organize columns in a standard format.\n",
    "\n",
    "- The class label should be stored in the first column.\n",
    "- The remaining specified columns correspond to features/predictors.\n",
    "- Our helper function will allow us to specify whether the data file contains a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_standard_df(\n",
    "        local_filepath;\n",
    "        header=false,\n",
    "        missingstring=\"\",\n",
    "        class_index::Integer=0,\n",
    "        feature_indices=1:0,\n",
    "    )\n",
    "    # Sanity checks.\n",
    "    if class_index < 1\n",
    "        error(\"class_index should be a positive integer.\")\n",
    "    end\n",
    "    if isempty(feature_indices) || any(<(0), feature_indices)\n",
    "        error(\"feature_indices should contain positive integers.\")\n",
    "    end\n",
    "    \n",
    "    # Read the input DataFrame.\n",
    "    input_df = CSV.read(local_filepath, DataFrame; header=header, missingstring=missingstring)\n",
    "    \n",
    "    # Initialize output DataFrame.\n",
    "    output_df = DataFrame()\n",
    "    \n",
    "    # Add the (first) column corresponding to class labels.\n",
    "    output_df[!, :class] = input_df[!, class_index]\n",
    "    \n",
    "    # Add the remaining columns corresponding to features/predictors.\n",
    "    for (i, feature_index) in enumerate(feature_indices)\n",
    "        column_name = Symbol(\"feature\", i)\n",
    "        output_df[!, column_name] = input_df[!, feature_index]\n",
    "    end\n",
    "\n",
    "    return output_df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124aad7",
   "metadata": {},
   "source": [
    "Test the function on our `iris` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = create_standard_df(@datadep_str(\"iris/iris.data\");\n",
    "    header=false,        # source does not have a header\n",
    "    class_index=5,       # class labels are stored in Column 5\n",
    "    feature_indices=1:4  # features are stored in Columns 1-4\n",
    ")\n",
    "first(iris_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0a6e1",
   "metadata": {},
   "source": [
    "Now let's define a helper function that processes the file `breast-cancer-wisconsin.data`.\n",
    "\n",
    "A few things to note (based on the notes in `breast-cancer.wisconsin.names`):\n",
    "   \n",
    "- There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "    (i.e., unavailable) attribute value, now denoted by \"?\".\n",
    "    \n",
    "- The structure of the `.data` file is:\n",
    "    >             #  Attribute                     Domain\n",
    "           -- -----------------------------------------\n",
    "           1. Sample code number            id number\n",
    "           2. Clump Thickness               1 - 10\n",
    "           3. Uniformity of Cell Size       1 - 10\n",
    "           4. Uniformity of Cell Shape      1 - 10\n",
    "           5. Marginal Adhesion             1 - 10\n",
    "           6. Single Epithelial Cell Size   1 - 10\n",
    "           7. Bare Nuclei                   1 - 10\n",
    "           8. Bland Chromatin               1 - 10\n",
    "           9. Normal Nucleoli               1 - 10\n",
    "          10. Mitoses                       1 - 10\n",
    "          11. Class:                        (2 for benign, 4 for malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd37027",
   "metadata": {},
   "outputs": [],
   "source": [
    "function process_bcw(local_filepath)\n",
    "    # First, let's standardize the DataFrame format. We'll drop the sample code number.\n",
    "    df = create_standard_df(local_filepath;\n",
    "        header=false,\n",
    "        missingstring=[\"?\",\"\"],\n",
    "        class_index=11,\n",
    "        feature_indices=2:10,\n",
    "    )\n",
    "    \n",
    "    # Next, change the class labels from 2 and 4 to benign and malignant, respectively.\n",
    "    # If we encounter another label, set it to missing.\n",
    "    new_label = Vector{Union{Missing,String}}(undef, length(df.class))\n",
    "    for (i, row) in enumerate(eachrow(df))\n",
    "        if row.class == 2\n",
    "            new_label[i] = \"benign\"\n",
    "        elseif row.class == 4\n",
    "            new_label[i] = \"malignant\"\n",
    "        else\n",
    "            new_label[i] = missing\n",
    "        end\n",
    "    end\n",
    "    df.class = new_label\n",
    "    \n",
    "    # Now drop any rows with missing values and set the type of every feature to Float64.\n",
    "    df = dropmissing(df)\n",
    "    for i in 2:ncol(df)\n",
    "        df[!,i] = map(xi -> Float64(xi), df[!,i])\n",
    "    end\n",
    "    \n",
    "    # Set column information.\n",
    "    column_info = Vector{String}(undef, 10)\n",
    "    column_info[1] = \"diagnosis\"\n",
    "    column_info[2] = \"clump_thickness\"\n",
    "    column_info[3] = \"cell_size_uniformity\"\n",
    "    column_info[4] = \"cell_shape_uniformity\"\n",
    "    column_info[5] = \"marginal_adhesion\"\n",
    "    column_info[6] = \"single_cell_epithelial_size\"\n",
    "    column_info[7] = \"bare_nuclei\"\n",
    "    column_info[8] = \"bland_chromatin\"\n",
    "    column_info[9] = \"normal_nucleoli\"\n",
    "    column_info[10] = \"mitoses\"\n",
    "    \n",
    "    column_info_df = DataFrame(columns=column_info)\n",
    "\n",
    "    # Finally, save our formatted data and remove the original source.\n",
    "    CSV.write(\"bcw.csv\", df; writeheader=true, delim=',')\n",
    "    CSV.write(\"bcw.info\", column_info_df; writeheader=false, delim=',')\n",
    "    rm(local_filepath)\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c216924",
   "metadata": {},
   "source": [
    "**Important** The `post_fetch_method` will run inside the `DataDep` directory, so we can safely write to the folder `datadep_name` without worrying about filepaths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5dd79",
   "metadata": {},
   "source": [
    "Now let's do the same for the file `wdbc.data`.\n",
    "\n",
    "> 1) ID number\n",
    ">\n",
    "> 2) Diagnosis (M = malignant, B = benign)\n",
    ">\n",
    "> 3-32) Ten real-valued features are computed for each cell nucleus:\n",
    ">\n",
    ">    a) radius (mean of distances from center to points on the perimeter)\n",
    ">\n",
    ">    b) texture (standard deviation of gray-scale values)\n",
    ">\n",
    ">    c) perimeter\n",
    ">\n",
    ">    d) area\n",
    ">\n",
    ">    e) smoothness (local variation in radius lengths)\n",
    ">\n",
    ">    f) compactness (perimeter^2 / area - 1.0)\n",
    ">\n",
    ">    g) concavity (severity of concave portions of the contour)\n",
    ">\n",
    ">    h) concave points (number of concave portions of the contour)\n",
    ">\n",
    ">    i) symmetry \n",
    ">\n",
    ">    j) fractal dimension (\"coastline approximation\" - 1)\n",
    ">\n",
    "> The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    ">\n",
    "> All feature values are recoded with four significant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "function process_wdbc(local_filepath)\n",
    "    # First, let's standardize the DataFrame format. We'll drop the ID number.\n",
    "    df = create_standard_df(local_filepath;\n",
    "        header=false,\n",
    "        missingstring=[\"?\",\"\"],\n",
    "        class_index=2,\n",
    "        feature_indices=3:32,\n",
    "    )\n",
    "    \n",
    "    # Next, change the class labels from 2 and 4 to benign and malignant, respectively.\n",
    "    # If we encounter another label, set it to missing.\n",
    "    new_label = Vector{Union{Missing,String}}(undef, length(df.class))\n",
    "    for (i, row) in enumerate(eachrow(df))\n",
    "        if row.class == \"B\"\n",
    "            new_label[i] = \"benign\"\n",
    "        elseif row.class == \"M\"\n",
    "            new_label[i] = \"malignant\"\n",
    "        else\n",
    "            new_label[i] = missing\n",
    "        end\n",
    "    end\n",
    "    df.class = new_label\n",
    "    \n",
    "    # Now drop any rows with missing values and set the type of every feature to Float64.\n",
    "    df = dropmissing(df)\n",
    "    for i in 2:ncol(df)\n",
    "        df[!,i] = map(xi -> Float64(xi), df[!,i])\n",
    "    end\n",
    "    \n",
    "    # Set column information. Columns are <feature>_<transformation>.\n",
    "    features = [\n",
    "        \"radius\",\n",
    "        \"texture\",\n",
    "        \"perimeter\",\n",
    "        \"area\",\n",
    "        \"smoothness\",\n",
    "        \"compactness\",\n",
    "        \"concavity\",\n",
    "        \"n_concave_pts\",\n",
    "        \"symmetry\",\n",
    "        \"fractal_dim\",\n",
    "    ]\n",
    "    transformations = [\"mean\", \"se\", \"worst\"]\n",
    "    \n",
    "    column_info = Vector{String}(undef, 31)\n",
    "    \n",
    "    idx = 1\n",
    "    column_info[idx] = \"diagnosis\"\n",
    "    idx += 1\n",
    "    \n",
    "    for transformation in transformations, feature in features\n",
    "        column_info[idx] = string(feature, \"_\", transformation)\n",
    "        idx += 1\n",
    "    end\n",
    "    \n",
    "    column_info_df = DataFrame(columns=column_info)\n",
    "    \n",
    "    # Finally, save our formatted data and remove the original source.\n",
    "    CSV.write(\"wdbc.csv\", df; writeheader=true, delim=',')\n",
    "    CSV.write(\"wdbc.info\", column_info_df; writeheader=false, delim=',')\n",
    "    rm(local_filepath)\n",
    "    \n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02777ff",
   "metadata": {},
   "source": [
    "Finally, because `post_fetch_method` only accepts a single function, let's stitch our functions together so that we can call the correct helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "function custom_post_fetch_method(local_filepath)\n",
    "    filename = basename(local_filepath)\n",
    "    if filename == \"breast-cancer-wisconsin.data\"\n",
    "        process_bcw(local_filepath)\n",
    "    elseif filename == \"wdbc.data\"\n",
    "        process_wdbc(local_filepath)\n",
    "    else\n",
    "        error(\"Did not specify a branch for $(filename).\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(DataDep(\n",
    "    # 1. Set the DataDep's name.\n",
    "    \"breast-cancer-wisconsin\",\n",
    "    # 2. Set the message to display when downloading.\n",
    "    \"\"\"\n",
    "    Dataset: breast-cancer-wisconsin\n",
    "    Author: Dr. WIlliam H. Wolberg\n",
    "    Donors: Olvi Mangasarian\n",
    "        Received by David W. Aha\n",
    "\n",
    "    This dataset contains two files, \"bcw.data\" and \"wdbc.data\" corresponding to \"breast-cancer-wisconsin.data\"\n",
    "    and \"wdbc.data\", respectively, in the UCI Machine Learning Repository.\n",
    "    \n",
    "    Summary for \"bcw.data\":\n",
    "        Observations: 699 (16 missing are dropped)\n",
    "        Features:     9\n",
    "        Classes:      2\n",
    "        \n",
    "    Summary for \"wdbc.data\"\n",
    "        Observations: 569 (0 missing)\n",
    "        Features:     30\n",
    "        Classes:      2\n",
    "    \n",
    "    Please see https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original).\n",
    "    \"\"\",\n",
    "    # 3. Set the remote_path used to download data.\n",
    "    [\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\",\n",
    "    ],\n",
    "    # 4. Set the checksum this time.\n",
    "    \"962af71216fdc2cbd457539d59cbadf9fbdc352a01831d8d79a9c5d1509b742e\";\n",
    "    # 5. Pass the function to run after downloading files.\n",
    "    post_fetch_method=custom_post_fetch_method\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = @datadep_str \"breast-cancer-wisconsin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573426a",
   "metadata": {},
   "source": [
    "What are the files inside the `DataDep`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d000e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`ls $(dir)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(joinpath(dir, \"bcw.csv\"), DataFrame)\n",
    "first(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f053343",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`cat -n $(joinpath(dir, \"bcw.info\"))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10649428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(@datadep_str(\"breast-cancer-wisconsin/wdbc.csv\"), DataFrame)\n",
    "first(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`cat -n $(joinpath(dir, \"wdbc.info\"))`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed682476",
   "metadata": {},
   "source": [
    "## Example 3: Reproducing datasets derived from simulations\n",
    "\n",
    "In this last example we create a `DataDep` for data generated by a simulation, implemented as a function called `spirals`.\n",
    "\n",
    "Here we are assuming that:\n",
    "\n",
    "- the `spirals` simulation can be shared with others (for example, people interested in reproducing our results), and\n",
    "- people that use the simulated data care about the parameters used to simulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf639ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function spirals(class_sizes;\n",
    "        rng::AbstractRNG=StableRNG(1903),\n",
    "        max_radius::Real=7.0,\n",
    "        x0::Real=-3.5,\n",
    "        y0::Real=3.5,\n",
    "        angle_start::Real=π/8,\n",
    "        prob::Real=1.0,\n",
    "    )\n",
    "    if length(class_sizes) != 3\n",
    "        error(\"Must specify 3 classes (length(class_sizes)=$(length(class_sizes))).\")\n",
    "    end\n",
    "    if max_radius <= 0\n",
    "        error(\"Maximum radius (max_radius=$(max_radius)) must be > 0.\")\n",
    "    end\n",
    "    if angle_start < 0\n",
    "        error(\"Starting angle (angle_start=$(angle_start)) should satisfy 0 ≤ θ ≤ 2π.\")\n",
    "    end\n",
    "    if prob < 0 || prob > 1\n",
    "        error(\"Probability (prob=$(prob)) must satisfy 0 ≤ prob ≤ 1.\")\n",
    "    end\n",
    "\n",
    "    # Extract parameters.\n",
    "    N = sum(class_sizes)\n",
    "    max_A, max_B, max_C = class_sizes\n",
    "\n",
    "    # Simulate the data.\n",
    "    L, X = Vector{String}(undef, N), Matrix{Float64}(undef, N, 2)\n",
    "    x, y = view(X, :, 1), view(X, :, 2)\n",
    "    inversions = 0\n",
    "    for i in 1:N\n",
    "        if i ≤ max_A\n",
    "            # The first 'max_A' samples are from Class A\n",
    "            (class, k, n, θ) = (\"A\", i, max_A, angle_start)\n",
    "            noise = 0.1\n",
    "        elseif i ≤ max_A + max_B\n",
    "            # The next 'max_B' samples are from Class B\n",
    "            (class, k, n, θ) = (\"B\", i-max_A+1, max_B, angle_start + 2π/3)\n",
    "            noise = 0.2\n",
    "        else\n",
    "            # The last 'max_C' samples are from Class C\n",
    "            (class, k, n, θ) = (\"C\", i-max_A-max_B+1, max_C, angle_start + 4π/3)\n",
    "            noise = 0.3\n",
    "        end\n",
    "\n",
    "        # Compute coordinates.\n",
    "        angle = θ + π * k / n\n",
    "        radius = max_radius * (1 - k / (n + n / 5))\n",
    "\n",
    "        x[i] = x0 + radius*cos(angle) + noise*randn(rng)\n",
    "        y[i] = y0 + radius*sin(angle) + noise*randn(rng)\n",
    "        if rand(rng) < prob\n",
    "            L[i] = class\n",
    "        else\n",
    "            L[i] = rand(rng, setdiff([\"A\", \"B\", \"C\"], [class]))\n",
    "            inversions += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println()\n",
    "    println(\"[  spirals: $(N) instances / 2 features / 3 classes  ]\")\n",
    "    println(\"  ∘ Pr(y | x) = $(prob)\")\n",
    "    println(\"  ∘ $inversions class inversions ($(inversions/N) Bayes error)\")\n",
    "    println()\n",
    "    \n",
    "    return L, X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, X = spirals([100, 100, 100])\n",
    "label2int = Dict(\"A\" => 1, \"B\" => 2, \"C\" => 3)\n",
    "class_colors = [label2int[li] for li in L]\n",
    "\n",
    "scatter(X[:,1], X[:,2], color=class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(DataDep(\n",
    "    # 1. Set the DataDep's name.\n",
    "    \"spirals\",\n",
    "    # 2. Set the message to display when downloading.\n",
    "    \"\"\"\n",
    "    Dataset: spirals\n",
    "    Credit: https://smorbieu.gitlab.io/generate-datasets-to-understand-some-clustering-algorithms-behavior/\n",
    "\n",
    "    A simulated dataset of three noisy spirals. Data is simulated locally; see registration block for details.\n",
    "\n",
    "    Observations: 1000\n",
    "    Features:     2\n",
    "    Classes:      3\n",
    "    \"\"\",\n",
    "    # 3. There is nothing to download, so this argument is not used.\n",
    "    \"unused\",\n",
    "    # 4. Specify the checksum of the simulation file, before running post_fetch_method.\n",
    "    \"42d5c3404511db5ab48ab2224ac2d2959c82a47dd4b108fbabb3dfb27631d782\";\n",
    "    # 5. Write a fetch_method that calls our simulation routine and creates a local file.\n",
    "    fetch_method = function(unused, localdir)\n",
    "        # Simulate the data. Note that we are forced to specify a RNG, its seed, and simulation parameters.\n",
    "        rng = StableRNG(1903)\n",
    "        L, X = spirals((600, 300, 100);\n",
    "            rng=rng,\n",
    "            max_radius=7.0,\n",
    "            x0=-3.5,\n",
    "            y0=3.5,\n",
    "            angle_start=pi/8,\n",
    "            prob=1.0,\n",
    "        )\n",
    "\n",
    "        # Put everything in a DataFrame.\n",
    "        x, y = view(X, :, 1), view(X, :, 2)\n",
    "        df = DataFrame(class=L, x=x, y=y)\n",
    "        \n",
    "        # Shuffle the rows of the DataFrame and write to file.\n",
    "        local_file = joinpath(localdir, \"data.csv\")\n",
    "        perm = Random.randperm(rng, size(df, 1))\n",
    "        foreach(col -> permute!(col, perm), eachcol(df))\n",
    "        CSV.write(local_file, df)\n",
    "        \n",
    "        return local_file\n",
    "    end,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@datadep_str \"spirals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ee9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(@datadep_str(\"spirals/data.csv\"), DataFrame)\n",
    "label2int = Dict(\"A\" => 1, \"B\" => 2, \"C\" => 3)\n",
    "class_colors = [label2int[li] for li in df.class]\n",
    "\n",
    "scatter(df.x, df.y, color=class_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447012c8",
   "metadata": {},
   "source": [
    "The point here is that we are forced to document the settings used in generating our data when we create a `DataDep` registration block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41cb25c",
   "metadata": {},
   "source": [
    "### Aside: Random Number Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4762d6",
   "metadata": {},
   "source": [
    "Random Number Generators (RNGs) on a computer are often *Pseudo*-Random Number Generators (PRNGs) in that the streams they generate are deterministic given you know some initial state.\n",
    "\n",
    "This property is great for making certain kinds of programs reproducible; for example simulations, probabilistic methods, and so on.\n",
    "\n",
    "**However, the implementation of a particular RNG algorithm may affect the random streams it generates!**\n",
    "\n",
    "- Minor bug fixes and/or speed improvements may affect streams.\n",
    "- This is acknowledged in the Julia documentation [here](https://docs.julialang.org/en/v1/stdlib/Random/#Random-Numbers).\n",
    "\n",
    "**Example**: At some point in the development of Julia, the default RNG was based on [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister). The default is now Xoshiro256++ from the [xoshiro/xoroshiro](https://en.wikipedia.org/wiki/Xorshift#xoshiro_and_xoroshiro) family. Scripts run in Julia versions from before the change that seeded the global RNG produce different sequences of values compared to more recent versions.\n",
    "\n",
    "- Your results should not depend on a particular stream of random numbers... but that reproducibility could be useful in diagnosing issues in an analysis or computational method.\n",
    "\n",
    "**If you absolutely need reproducibility of random numbers in a script, use a RNG that promises stability.**\n",
    "\n",
    "- In Julia: [StableRNGs](https://github.com/JuliaRandom/StableRNGs.jl) promises stability across Julia versions.\n",
    "- Other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bcdfa",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Use DataDeps.jl to\n",
    "\n",
    "1. document and help reproduce processes used in data wrangling, and\n",
    "2. manage local storage of ready-to-use data.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "The self-directed exerccises will walk you through additional examples, including managing compressed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
